{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9477d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#General imports\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn.cluster import DBSCAN\n",
    "from astropy.stats import SigmaClip\n",
    "from photutils.background import SExtractorBackground\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "#BANZAI pipeline imports\n",
    "import requests\n",
    "from banzai.calibrations import make_master_calibrations\n",
    "from banzai_nres import settings\n",
    "from banzai import dbs\n",
    "from banzai.utils.stage_utils import run_pipeline_stages\n",
    "from banzai.logs import set_log_level\n",
    "from banzai.context import Context\n",
    "import logging\n",
    "from banzai_nres.frames import NRESFrameFactory\n",
    "from banzai.data import DataProduct\n",
    "from banzai_nres.wavelength import IdentifyFeatures\n",
    "from banzai_nres.flats import FlatLoader\n",
    "from banzai_nres.wavelength import ArcLoader, LineListLoader, WavelengthCalibrate\n",
    "from banzai_nres.qc.qc_wavelength import AssessWavelengthSolution\n",
    "\n",
    "#SQAT imports\n",
    "import matplotlib.dates as mdates\n",
    "from photutils.segmentation import make_2dgaussian_kernel\n",
    "from photutils.background import Background2D, MedianBackground\n",
    "from photutils.utils import calc_total_error\n",
    "from photutils.segmentation import detect_sources\n",
    "from photutils.segmentation import SourceCatalog\n",
    "from photutils.segmentation import deblend_sources\n",
    "from astropy.convolution import convolve\n",
    "from photutils.background import MADStdBackgroundRMS\n",
    "from banzai.utils.stats import robust_standard_deviation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a561a",
   "metadata": {},
   "source": [
    "# SQAT (Spectrum Quality Analysis Tool)\n",
    "\n",
    "This class (should) provide everything you need to analyse NRES spectra. An example of what running it might look like is shown below the class.\n",
    "\n",
    "## NOTE:\n",
    "To run a developer version of the BANZAI NRES pipeline, i.e. whatever version of code you are working on, install the pipeline as so:\n",
    "```\n",
    "pip install -e /path/to/banzai-nres\n",
    "```\n",
    "\n",
    "Switching branches in git will switch the version of code read by SQAT\n",
    "\n",
    "\n",
    "Still working on customizability of plots a little, feel free to edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef83ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQAT():\n",
    "    \n",
    "    def __init__(self, files=None):\n",
    "        if files:\n",
    "            self.files = sorted(files)\n",
    "            self.lampflats_path = None\n",
    "            self.doubles_path = None\n",
    "            self.context_path = None\n",
    "            \n",
    "    def download_data(self, start_date=None, end_date=None, site=None, doubles_path=None, lampflats_path=None, stacked=True):\n",
    "        self.stacked = stacked\n",
    "        self.site = site\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.doubles_path = doubles_path\n",
    "        self.lampflats_path = lampflats_path\n",
    "        #check format of dates\n",
    "        if start_date and end_date:\n",
    "            try:\n",
    "                time.strptime(self.start_date, '%Y-%m-%d')\n",
    "                time.strptime(self.end_date, '%Y-%m-%d')\n",
    "            except ValueError:\n",
    "                print('Please format input dates as YYYY-mm-dd')\n",
    "        #check that site is part of list of sites\n",
    "        sites_dict = {'sites': ['lsc', 'tlv', 'elp']}\n",
    "        if site:\n",
    "            if not self.site in sites_dict['sites']:\n",
    "                print(f'Please choose a site from the following: {sites_dict[\"sites\"]}')\n",
    "        #check path format\n",
    "        if doubles_path and lampflats_path:\n",
    "            if self.doubles_path[-1] == '/':\n",
    "                self.doubles_path = self.doubles_path[:-1]\n",
    "            if self.lampflats_path[-1] == '/':\n",
    "                self.lampflats_path = self.lampflats_path[:-1]\n",
    "                \n",
    "        #To get stacked doubles\n",
    "        if self.start_date and self.end_date and self.site and self.doubles_path and self.lampflats_path and self.stacked==True:\n",
    "            #query archive for frames\n",
    "            archive_record = requests.get(f'https://archive-api.lco.global/frames/?reduction_level=92&site_id={self.site}&configuration_type=DOUBLE&basename=double-bin1x1-110&start={self.start_date}&end={self.end_date}&public=true').json()['results']\n",
    "            for rec in archive_record:\n",
    "                #Give path to write files to\n",
    "                with open(f'{self.doubles_path}/{rec[\"filename\"]}', 'wb') as f:\n",
    "                    f.write(requests.get(rec['url']).content)\n",
    "            #get associated lampflats\n",
    "            files = glob(f'{self.doubles_path}/*.fz', recursive=True)\n",
    "            for file in files:\n",
    "                # get double and associated stacked lampflat\n",
    "                double_frame = fits.open(file)\n",
    "                super_flat = double_frame['SPECTRUM'].header.get('L1IDFLAT')[:-8] # gives the basename of the lampflat\n",
    "                if super_flat is not None:\n",
    "                    # query archive for frame\n",
    "                    archive_record = requests.get(f'https://archive-api.lco.global/frames/?basename_exact={super_flat}').json()['results'][0]\n",
    "                    # write frame to disk\n",
    "                    with open(f'{self.lampflats_path}/{archive_record[\"filename\"]}', 'wb') as f:\n",
    "                        f.write(requests.get(archive_record['url']).content)\n",
    "        \n",
    "        #To get unstacked doubles\n",
    "        elif self.start_date and self.end_date and self.site and self.doubles_path and self.lampflats_path and self.stacked==False:\n",
    "            #query archive for frames\n",
    "            archive_record = requests.get(f'https://archive-api.lco.global/frames/?reduction_level=92&site_id={self.site}&configuration_type=DOUBLE&basename=a92&start={self.start_date}&end={self.end_date}&public=true').json()['results']\n",
    "            for rec in archive_record:\n",
    "                #Give path to write files to\n",
    "                with open(f'{self.doubles_path}/{rec[\"filename\"]}', 'wb') as f:\n",
    "                    f.write(requests.get(rec['url']).content)\n",
    "            #get associated lampflats\n",
    "            files = glob(f'{self.doubles_path}/*.fz', recursive=True)\n",
    "            for file in files:\n",
    "                # get double and associated stacked lampflat\n",
    "                double_frame = fits.open(file)\n",
    "                super_flat = double_frame['SPECTRUM'].header.get('L1IDFLAT')[:-8] # gives the basename of the lampflat\n",
    "                print(super_flat)\n",
    "                if super_flat is not None:\n",
    "                    # query archive for frame\n",
    "                    archive_record = requests.get(f'https://archive-api.lco.global/frames/?basename_exact={super_flat}').json()['results'][0]\n",
    "                    # write frame to disk\n",
    "                    with open(f'{self.lampflats_path}/{archive_record[\"filename\"]}', 'wb') as f:\n",
    "                        f.write(requests.get(archive_record['url']).content)\n",
    "            \n",
    "        else:\n",
    "            print('Please Provide the date range of interest (YYYY-MM-DD), site name (eg. \"lsc\"), and paths to write files')\n",
    "    \n",
    "    def setup_pipeline(self, processed_path, db_path=None):\n",
    "        print('Please do not run this more than one time. Database is set up in your current directory if not provided')     \n",
    "        set_log_level('DEBUG')\n",
    "        logger = logging.getLogger('banzai')\n",
    "        \n",
    "        self.db_path = db_path\n",
    "        if self.db_path:\n",
    "            if self.db_path[-1] == '/':\n",
    "                self.db_path = self.db_path[:-1]\n",
    "                os.environ['DB_ADDRESS'] = f'sqlite:///{db_path}/test.db'\n",
    "        else:\n",
    "            os.environ['DB_ADDRESS'] = 'sqlite:///test.db'\n",
    "        os.environ['CONFIGDB_URL'] = 'http://configdb.lco.gtn/sites'\n",
    "        os.environ['OPENTSDB_PYTHON_METRICS_TEST_MODE'] = 'True'\n",
    "        os.system(f'banzai_nres_create_db --db-address={os.environ[\"DB_ADDRESS\"]}')\n",
    "        \n",
    "        settings.processed_path= os.path.join(os.getcwd(), 'test_data')\n",
    "        settings.fpack=True\n",
    "        settings.db_address = os.environ['DB_ADDRESS']\n",
    "        settings.reduction_level = 92\n",
    "        \n",
    "        # set up the context object.\n",
    "        import banzai.main\n",
    "        context = banzai.main.parse_args(settings, parse_system_args=False)\n",
    "        context = vars(context)\n",
    "        context['no_bpm'] = True \n",
    "        context['processed_path'] = processed_path\n",
    "        context['post_to_archive'] = False\n",
    "        context['no_file_cache'] = False\n",
    "        self.context = Context(context)\n",
    "        \n",
    "        # initialize the DB with some instruments from ConfigDB\n",
    "        \n",
    "        os.system(f'banzai_nres_create_db --db-address={os.environ[\"DB_ADDRESS\"]}')\n",
    "        os.system(f'banzai_update_db --db-address={os.environ[\"DB_ADDRESS\"]} --configdb-address={os.environ[\"CONFIGDB_URL\"]}')\n",
    "        \n",
    "        # wow, after all that you can actually open an image!\n",
    "        \n",
    "        self.frame_factory = NRESFrameFactory()\n",
    "    \n",
    "    def run_pipeline(self, lampflats_path=None, doubles_path=None):\n",
    "        #Load flats in\n",
    "        try:\n",
    "            self.lampflats_path\n",
    "        except AttributeError:\n",
    "            if not lampflats_path:\n",
    "                print('Please provide a path to your lampflats')\n",
    "                sys.exit(-1)\n",
    "            else:\n",
    "                self.lampflats_path = lampflats_path\n",
    "        \n",
    "        try:\n",
    "            self.doubles_path\n",
    "        except AttributeError:\n",
    "            if not doubles_path:\n",
    "                print('Please provide a path to your doubles')\n",
    "                sys.exit(-1)\n",
    "            else:\n",
    "                self.doubles_path = doubles_path\n",
    "        \n",
    "        try:\n",
    "            self.context\n",
    "        except AttributeError:\n",
    "            print('Please run the .setup_pipeline method before running the pipeline')\n",
    "            sys.exit(-1)\n",
    "            \n",
    "        frame_factory = NRESFrameFactory()\n",
    "        lamps = glob(f'{self.lampflats_path}/*.fz', recursive=True)\n",
    "        lamps = sorted(lamps)\n",
    "        for image_path in lamps:\n",
    "            cal_image = frame_factory.open({'path': image_path}, self.context)\n",
    "            dbs.save_calibration_info(cal_image.to_db_record(DataProduct(None, filename=os.path.basename(image_path),\n",
    "                                                                               filepath=os.path.dirname(image_path))),\n",
    "                                                                               os.environ['DB_ADDRESS'])\n",
    "        feature_stage = IdentifyFeatures(self.context)\n",
    "        flat_stage = FlatLoader(self.context)\n",
    "        arc_stage = ArcLoader(self.context)\n",
    "        line_list_stage = LineListLoader(self.context)\n",
    "        wavelength_stage = WavelengthCalibrate(self.context)\n",
    "        qc_stage = AssessWavelengthSolution(self.context)\n",
    "\n",
    "        files = glob(f'{self.doubles_path}/*.fz', recursive=True)\n",
    "        files = sorted(files)\n",
    "\n",
    "        for path in files:\n",
    "            image = frame_factory.open({'path': path}, self.context)\n",
    "            image = flat_stage.do_stage(image) # should pull info from the flats on disk and add them to the image\n",
    "            image = arc_stage.do_stage(image)\n",
    "            image = line_list_stage.do_stage(image) #should pull info from the line list and add them to the image\n",
    "            image = feature_stage.do_stage(image) #Create the features table\n",
    "            image = wavelength_stage.do_stage(image) #should now have all the data it needs!\n",
    "            image = qc_stage.do_stage(image)\n",
    "            \n",
    "            image.write(self.context)\n",
    "           \n",
    "    def get_data(self, path):\n",
    "        if path[-1] == '/':\n",
    "            path = path[:-1]\n",
    "        self.stacked_files = glob(f'{path}/*.fz', recursive=True)\n",
    "        self.header = [fits.open(f)['SPECTRUM'].header for f in self.stacked_files]\n",
    "        self.times = [h['DAY-OBS'][-4:] for h in self.header]\n",
    "        return\n",
    "    \n",
    "    def get_sources_old(self, max_dist=1, min_cluster_size=20):\n",
    "        self.features = [pd.DataFrame(fits.open(f)['FEATURES'].data) for f in self.stacked_files]\n",
    "        #First concatenate tables\n",
    "        [f.insert(14, 'date-obs', [self.times[i]]*len(f)) for i, f in enumerate(self.features)]\n",
    "        feature_stack = pd.concat(self.features)\n",
    "\n",
    "        #Then collate using DBSCAN\n",
    "        clustering = DBSCAN(eps = max_dist, min_samples = min_cluster_size).fit_predict(feature_stack.loc[:, ['id', 'xcentroid', 'ycentroid', 'wavelength', 'fiber', 'order']].to_numpy())\n",
    "\n",
    "        feature_stack.insert(15, 'labels', clustering)\n",
    "        feature_stack.sort_values('labels')\n",
    "        sources = [v.reset_index() for k, v in feature_stack.groupby('labels')]\n",
    "\n",
    "        #Fill in missing dates with nans\n",
    "        for s in sources[1:]:\n",
    "            missing = list(set(self.times).difference(s['date-obs']))\n",
    "            for m in missing:\n",
    "                s.loc[len(s)] = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, m, np.nan]\n",
    "                s.sort_values('date-obs', inplace=True)\n",
    "            s.drop(['labels', 'index'], inplace=True, axis=1)\n",
    "            s.rename(columns={'id':'line'})\n",
    "        \n",
    "        self.sources = sources\n",
    "        return\n",
    "    \n",
    "    def get_sources_new(self, max_dist=1, min_cluster_size=20):\n",
    "        self.features = [pd.DataFrame(fits.open(f)['FEATURES'].data) for f in self.stacked_files]\n",
    "        #First concatenate tables\n",
    "        [f.insert(12, 'date-obs', [self.times[i]]*len(f)) for i, f in enumerate(self.features)]\n",
    "        feature_stack = pd.concat(self.features)\n",
    "\n",
    "        #Then collate using DBSCAN\n",
    "        clustering = DBSCAN(eps = max_dist, min_samples = min_cluster_size).fit_predict(feature_stack.loc[:, ['id', 'xcentroid', 'ycentroid', 'wavelength', 'fiber', 'order']].to_numpy())\n",
    "\n",
    "        feature_stack.insert(12, 'labels', clustering)\n",
    "        feature_stack.sort_values('labels')\n",
    "        sources = [v.reset_index() for k, v in feature_stack.groupby('labels')]\n",
    "\n",
    "        #Fill in missing dates with nans\n",
    "        for s in sources[1:]:\n",
    "            missing = list(set(self.times).difference(s['date-obs']))\n",
    "            for m in missing:\n",
    "                s.loc[len(s)] = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, m]\n",
    "                s.sort_values('date-obs', inplace=True)\n",
    "            s.drop(['labels', 'index'], inplace=True, axis=1)\n",
    "            s.rename(columns={'id':'line'})\n",
    "        \n",
    "        self.sources = sources\n",
    "        return\n",
    "    \n",
    "    def ds9_to_python_id(self, coords):\n",
    "        new = []\n",
    "        if coords==None:\n",
    "            raise ValueError('Please Provide set of ds9 coordinates')\n",
    "        else:\n",
    "            pass\n",
    "        try:\n",
    "            for coord in coords:\n",
    "                new.append([coord[0]-1, coord[1]-1])\n",
    "        except TypeError:\n",
    "            new = [coords[0]-1, coords[1]-1]\n",
    "        return new\n",
    "\n",
    "    def search(self, coords, tbl, r=3):\n",
    "        #Search on coord at a time\n",
    "        new_coords = self.ds9_to_python_id(coords)\n",
    "        table = tbl.dropna()\n",
    "        x_table = np.mean(table['xcentroid'])\n",
    "        y_table = np.mean(table['ycentroid'])\n",
    "        try:\n",
    "            res = []\n",
    "            for coord in new_coords:\n",
    "                if (coord[0] - x_table)**2 + (coord[1] - y_table)**2 <= r**2:\n",
    "                    res.append(tbl)\n",
    "        except TypeError:\n",
    "            if (new_coords[0] - x_table)**2 + (new_coords[1] - y_table)**2 <= r**2:\n",
    "                res = tbl\n",
    "        return res\n",
    "    \n",
    "    def extract(self):\n",
    "        bpm_mask = np.array(fits.open(self.stacked_files[0])['BPM'].data, dtype=bool)\n",
    "        tables = []\n",
    "        file_data = [fits.open(f)['SPECTRUM'].data for f in self.stacked_files]\n",
    "        for data in file_data:\n",
    "            #Background estimation\n",
    "            #bkg_estimator = MedianBackground()\n",
    "            bkg = Background2D(data, (50, 50), filter_size=(3, 3)) #This is just an example not actual background estimation\n",
    "            data -= bkg.background  # subtract the background\n",
    "            data -= bkg.background_rms\n",
    "            threshold = 1.5*bkg.background_rms\n",
    "            \n",
    "            #Image segmentation\n",
    "            kernel = make_2dgaussian_kernel(5, size=3, mode='center')\n",
    "            convolved_data = convolve(data, kernel)\n",
    "            \n",
    "            #Get total error\n",
    "            err = calc_total_error(data, bkg.background_rms, 1)\n",
    "            \n",
    "            print('Making a segment map')\n",
    "            print('Masking')\n",
    "            segment_map = detect_sources(convolved_data, threshold, npixels = 5, connectivity = 4, mask = bpm_mask)\n",
    "            \n",
    "            print('Deblending')\n",
    "            segm_deblend = deblend_sources(convolved_data, segment_map, npixels=5, nlevels=64, contrast=0.001, progress_bar=False)\n",
    "            segment_map = segm_deblend\n",
    "            cat = SourceCatalog(data, segment_map, convolved_data=convolved_data, error=err)\n",
    "            xerr = np.sqrt(cat.covar_sigx2)\n",
    "            yerr = np.sqrt(cat.covar_sigy2)\n",
    "            tbl = cat.to_table()\n",
    "            tbl.add_columns([xerr, yerr], names = ['xcentroid_err', 'ycentroid_err'])\n",
    "            tables.append(tbl.to_pandas())\n",
    "        \n",
    "        header = [fits.open(f)['SPECTRUM'].header for f in self.stacked_files]\n",
    "        times = [h['DAY-OBS'][-4:] for h in header]\n",
    "\n",
    "        self.features = [t.drop(t.iloc[:, [0,3,4,5,6,7,8,9,10,11,13,14,15,16,17,18,19]], axis = 1) for t in tables]\n",
    "        [f.insert(0, 'date-obs', [times[i]]*len(f)) for i, f in enumerate(self.features)]\n",
    "\n",
    "        feature_stack = pd.concat(self.features)\n",
    "\n",
    "        #Then collate using DBSCAN\n",
    "        clustering = DBSCAN(eps = 1, min_samples = 20).fit_predict(feature_stack.loc[:, ['xcentroid', 'ycentroid', 'eccentricity']].to_numpy())\n",
    "\n",
    "\n",
    "        feature_stack.insert(0, 'labels', clustering)\n",
    "        feature_stack.sort_values('labels')\n",
    "        sources = [v.reset_index() for k, v in feature_stack.groupby('labels')]\n",
    "\n",
    "        #Fill in missing dates with nans\n",
    "        for s in sources[1:]:\n",
    "            missing = list(set(times).difference(s['date-obs']))\n",
    "            for m in missing:\n",
    "                s.loc[len(s)] = [np.nan, np.nan, m, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "                s.sort_values('date-obs', inplace=True)\n",
    "            s.drop(['labels', 'index'], inplace=True, axis=1)\n",
    "        self.sources = sources\n",
    "        return\n",
    "    \n",
    "    #-------------------PLOTTING--------------------------------\n",
    "    plt.style.use('seaborn')\n",
    "    \n",
    "    def wavelength_std_plot(self, clim = None):\n",
    "        wave = [fits.open(f)['WAVELENGTH'].data for f in self.stacked_files]\n",
    "        stdim = np.std(wave, axis = 0)\n",
    "        median = np.median(stdim)\n",
    "        std = scipy.stats.median_abs_deviation(stdim, axis=None)\n",
    "        if clim:\n",
    "            plt.imshow(stdim, origin='lower', cmap = 'inferno', clim=clim)\n",
    "        else:\n",
    "            plt.imshow(stdim, origin='lower', cmap = 'inferno', clim=[median - 3*std, median+3*std])\n",
    "        plt.grid(visible=False)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        \n",
    "        plt.hist(stdim.flatten(), range=(0.0003, .015), bins = 100)\n",
    "        plt.title('Noise Distribution')\n",
    "        plt.xlabel('Standard Deviation (pixel)')\n",
    "        plt.ylabel('Number of wavellengths')\n",
    "        plt.show()\n",
    "    \n",
    "    def features_std(self):\n",
    "        std = []\n",
    "        for tbl in self.sources[1:]:\n",
    "            wavelength = tbl.dropna()['wavelength']\n",
    "            if len(wavelength)>1:\n",
    "                std.append(np.std(wavelength))\n",
    "        plt.title('Standard Deviation of calculated wavelengths of features')\n",
    "        plt.xlabel('Standard Deviation')\n",
    "        plt.ylabel('Number of Features')\n",
    "        plt.hist(std, bins = int(len(self.sources)/100))\n",
    "        plt.show()\n",
    "    \n",
    "    def get_some_features(self, coords):\n",
    "        self.found = []\n",
    "        try:\n",
    "            for coord in coords:\n",
    "                for tbl in self.sources:\n",
    "                    finds = self.search(coord, tbl, r=5)\n",
    "                    if len(finds)!=0:\n",
    "                        self.found.append(finds)\n",
    "        except TypeError:\n",
    "            for tbl in self.sources:\n",
    "                finds = self.search(coord, tbl, r=5)\n",
    "                if len(finds)!=0:\n",
    "                    self.found.append(finds)\n",
    "        return self.found\n",
    "                    \n",
    "    def plot_feature_coords(self, coords):\n",
    "        plt.style.use('seaborn')\n",
    "       \n",
    "        t0 = datetime.strptime(self.times[0], '%m%d')\n",
    "        t1 = datetime.strptime(self.times[-1], '%m%d')\n",
    "        for f in self.found:\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(1,  3, figsize=(20,5), sharey=False)\n",
    "            x = f['date-obs']\n",
    "            locator = mdates.AutoDateLocator(minticks=len(x), maxticks=len(x)).get_locator(t0, t1)\n",
    "            ax1.errorbar(x, f['xcentroid']-np.nanmedian(f['xcentroid']), yerr = f['centroid_err'], ms = 6, fmt='.')\n",
    "            ax2.errorbar(x, f['ycentroid']-np.nanmedian(f['ycentroid']), yerr = f['centroid_err'], ms = 6, fmt='.')\n",
    "            ax3.errorbar(x, f['wavelength']-np.nanmedian(f['wavelength']), yerr = None, ms = 6, fmt='.')\n",
    "            \n",
    "            ax1.set_title('x position of ThAr cal points '+ str(f.dropna()['xcentroid'][0]))\n",
    "            ax2.set_title('y position of ThAr cal points ' + str(f.dropna()['ycentroid'][0]))\n",
    "            ax3.set_title('wavelength of feature ' + str(f.dropna()['wavelength'][0]))\n",
    "            \n",
    "            ax1.tick_params(axis = 'x', rotation=70, labelsize=7)\n",
    "            ax2.tick_params(axis = 'x', rotation=70, labelsize=7)\n",
    "            ax3.tick_params(axis = 'x', rotation=70, labelsize=7)\n",
    "            ax1.xaxis.set_major_locator(locator)\n",
    "            ax1.xaxis.set_minor_locator(locator)\n",
    "            ax2.xaxis.set_major_locator(locator)\n",
    "            ax2.xaxis.set_minor_locator(locator)\n",
    "            ax3.xaxis.set_major_locator(locator)\n",
    "            ax3.xaxis.set_minor_locator(locator)\n",
    "            plt.setp((ax1, ax2), ylabel='pixel value-median', )\n",
    "            plt.setp(ax3, ylabel = 'wavelength - median')\n",
    "            plt.show()\n",
    "        \n",
    "    def plot_extracted_coords(self, coords):\n",
    "        \n",
    "        plt.style.use('seaborn')\n",
    "        for f in self.found:\n",
    "            fig, (ax1, ax2) = plt.subplots(1,  2, figsize=(20,5), sharey=False)\n",
    "            x = f['date-obs']\n",
    "            ax1.errorbar(x, f['xcentroid']-np.nanmedian(f['xcentroid']), ms = 6, fmt='.')\n",
    "            ax2.errorbar(x, f['ycentroid']-np.nanmedian(f['ycentroid']), ms = 6, fmt='.')\n",
    "            \n",
    "            ax1.set_title('x position of ThAr cal points '+ str(f.dropna()['xcentroid'][0]))\n",
    "            ax2.set_title('y position of ThAr cal points ' + str(f.dropna()['ycentroid'][0]))\n",
    "            \n",
    "            ax1.tick_params(axis = 'x', rotation=70, labelsize=7)\n",
    "            ax2.tick_params(axis = 'x', rotation=70, labelsize=7)\n",
    "            plt.setp((ax1, ax2), ylabel='pixel value-median', xlabel= 'Date-Obs')\n",
    "            plt.show()\n",
    "    \n",
    "    def centroid_std_map(self, cmin=0, cmax=0.5):\n",
    "        plt.style.use('seaborn')\n",
    "        xcentroid_stds = []\n",
    "        ycentroid_stds = []\n",
    "        x=[]\n",
    "        y=[]\n",
    "        for s in self.sources[1:]:\n",
    "            xcentroid_stds.append(np.std(s['xcentroid']))\n",
    "            ycentroid_stds.append(np.std(s['ycentroid']))\n",
    "            x.append(np.nanmean(s['xcentroid']))\n",
    "            y.append(np.nanmean(s['ycentroid']))\n",
    "        plt.style.use('dark_background')\n",
    "        plt.figure(figsize=(30,30))\n",
    "        plt.scatter(x, y, c=xcentroid_stds, cmap='inferno', vmin=cmin, vmax=cmax)\n",
    "        plt.axis('off')\n",
    "        plt.grid('off')\n",
    "        cbar = plt.colorbar()\n",
    "        tick_font_size = 30\n",
    "        cbar.ax.tick_params(labelsize=tick_font_size)\n",
    "        plt.show()\n",
    "    \n",
    "    def rmse(self, data):\n",
    "        mean = np.nanmean(data)\n",
    "        return np.sqrt(np.sum((data-mean)**2)/len(data))\n",
    "    \n",
    "    def centroid_rmse_plot(self, x_range = (0,1)):\n",
    "        rmse_x = []\n",
    "        rmse_y = []\n",
    "        for s in self.sources[1:]:\n",
    "            rmse_x.append(self.rmse(s['xcentroid'].to_numpy()))\n",
    "            rmse_y.append(self.rmse(s['ycentroid'].to_numpy()))\n",
    "    \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12), sharex=True)\n",
    "        ax1.hist(rmse_x, bins = 100, label='xcentroid', range=x_range)\n",
    "        ax2.hist(rmse_y, bins = 100, label='ycentroid', range=x_range)\n",
    "        ax1.legend()\n",
    "        ax2.legend()\n",
    "        plt.setp((ax1, ax2), ylabel='Number of Features')\n",
    "        ax1.set_title('RMSE of x and y of Features')\n",
    "        ax2.set_xlabel('RMSE')\n",
    "        plt.show()\n",
    "    \n",
    "    def violin_plot(self, vmax=0.3, vmin=-0.3):\n",
    "        dfs = []\n",
    "\n",
    "        for source in self.sources[1:]:\n",
    "            center_mean = [np.mean(stats.sigmaclip(source['xcentroid'].dropna(), 3, 3)[0]), np.mean(stats.sigmaclip(source['ycentroid'].dropna(), 3, 3)[0]), np.mean(stats.sigmaclip(source['wavelength'].dropna(), 3, 3)[0])]\n",
    "            x_dist = []\n",
    "            y_dist = []\n",
    "            wave_dist = []\n",
    "            for i in np.arange(len(source)):\n",
    "                x_source = source.iloc[i]['xcentroid']\n",
    "                y_source = source.iloc[i]['ycentroid']\n",
    "                wave = source.iloc[i]['wavelength']\n",
    "                if not np.isnan(x_source):\n",
    "                    x_dist.append(x_source-center_mean[0])\n",
    "                    y_dist.append(y_source-center_mean[1])\n",
    "                    wave_dist.append(wave-center_mean[2])\n",
    "                else:\n",
    "                    x_dist.append(np.nan)\n",
    "                    y_dist.append(np.nan)\n",
    "                    wave_dist.append(np.nan)\n",
    "            #Make a new dataframe out of this\n",
    "            df = pd.DataFrame({'date-obs':self.times, 'xdiffs':x_dist,'ydiffs':y_dist, 'wavediffs':wave_dist})\n",
    "            dfs.append(df)\n",
    "\n",
    "        #Concatenate all the difference dfs\n",
    "        diff_df = pd.concat(dfs)\n",
    "        diff_df = pd.melt(diff_df, id_vars=['date-obs'], value_vars=['xdiffs', 'ydiffs', 'wavediffs'], var_name='category', value_name = 'diffs')\n",
    "        diff_df = diff_df[(diff_df['diffs'] >= vmin)&(diff_df['diffs'] <= vmax)]\n",
    "\n",
    "        import seaborn\n",
    "        seaborn.set(style='whitegrid', font_scale=2)\n",
    "\n",
    "        seaborn.set(style='whitegrid', rc={\"figure.figsize\":(40, 20)}, font_scale=2)\n",
    "        fig, axes = plt.subplots(3, 1, figsize = (40,20))\n",
    "        palette = {'xdiffs':'tab:blue', 'ydiffs':'tab:orange', 'wavediffs':'tab:green'}\n",
    "        seaborn.violinplot(x='date-obs', y='diffs', data=diff_df.loc[diff_df['category']=='xdiffs'], scale='count', hue='category', palette=palette, cut=0, bw=0.1, ax=axes[0])\n",
    "        seaborn.violinplot(x='date-obs', y='diffs', data=diff_df.loc[diff_df['category']=='ydiffs'], scale='count',  hue='category', palette=palette, cut=0, bw=0.1, ax=axes[1])\n",
    "        g = seaborn.violinplot(x='date-obs', y='diffs', data=diff_df.loc[diff_df['category']=='wavediffs'], scale='count',  hue='category', palette=palette, cut=0, bw=0.1, ax=axes[2])\n",
    "        g.set(ylim=(-0.01,0.01))\n",
    "        axes[0].set_title('x centroid variation')\n",
    "        axes[1].set_title('y centroid variation')\n",
    "        axes[2].set_title('wavelength variation')\n",
    "        plt.show()\n",
    "    \n",
    "    def signaltonoise(self, data, sigma=3.0):\n",
    "        sigma_clip = SigmaClip(sigma=3.0)        \n",
    "        bkgrms = MADStdBackgroundRMS(sigma_clip)\n",
    "        noise = bkgrms(data)\n",
    "        snr = np.sqrt((robust_standard_deviation(data)**2/noise**2) - 1)\n",
    "        return snr\n",
    "    \n",
    "    def rvprecsn(self):\n",
    "        self.rv = [h['RVPRECSN'] for h in self.header]\n",
    "        plt.style.use('seaborn')\n",
    "        plt.plot(self.times, self.rv)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Times')\n",
    "        plt.ylabel('RVPRECSN')\n",
    "        plt.title('LSC RV Precision Comparison')\n",
    "        plt.tick_params(axis = 'x', rotation=70, labelsize=10)\n",
    "        plt.show()\n",
    "    \n",
    "    def cal_and_sci(self, coords):\n",
    "        cal = []\n",
    "        sci = []\n",
    "        for s in self.sources[1:]:\n",
    "            if np.nanmean(s['fiber']) == 0:\n",
    "                cal.append(s)\n",
    "            elif np.nanmean(s['fiber']) == 1:\n",
    "                sci.append(s)\n",
    "\n",
    "        #Then do a radial search looking for each sci for each cal\n",
    "        def radial_search(comp_tbl, tbls, r=5):\n",
    "            for tbl in tbls:\n",
    "                coord = [np.nanmean(comp_tbl['xcentroid']), np.nanmean(comp_tbl['ycentroid'])]\n",
    "                x = np.nanmean(tbl['xcentroid'])\n",
    "                y = np.nanmean(tbl['ycentroid'])\n",
    "                if np.abs(coord[0] - x) + np.abs(coord[1] - y) <= r:  \n",
    "                    return tbl\n",
    "                else:\n",
    "                    pass\n",
    "        res = []\n",
    "        for t in cal:\n",
    "            res.append(radial_search(t, sci, r=20))\n",
    "\n",
    "        for i in np.arange(len(cal))[:50]:\n",
    "            if res[i] is not None:\n",
    "                fig, (ax1, ax2, ax3) = plt.subplots(1,  3, figsize=(20,5), sharey=False)\n",
    "                x = cal[i]['date-obs']\n",
    "                ax1.errorbar(x, cal[i]['xcentroid']-np.nanmedian(cal[i]['xcentroid']), ms = 6, fmt='.', label='cal fiber')\n",
    "                ax1.errorbar(x, res[i]['xcentroid']-np.nanmedian(res[i]['xcentroid']), ms = 6, mfc = 'red',fmt='.', label='sci fiber')\n",
    "                ax2.errorbar(x, cal[i]['ycentroid']-np.nanmedian(cal[i]['ycentroid']), ms = 6, fmt='.', label = 'cal fiber')\n",
    "                ax2.errorbar(x, res[i]['ycentroid']-np.nanmedian(res[i]['ycentroid']), ms = 6, mfc = 'red',fmt='.', label = 'sci fiber')\n",
    "                ax3.errorbar(x, cal[i]['wavelength']-np.nanmedian(cal[i]['wavelength']), yerr = None, ms = 6, fmt='.', label = 'cal fiber')\n",
    "                ax3.errorbar(x, res[i]['wavelength']-np.nanmedian(res[i]['wavelength']), yerr = None, ms = 6, mfc='red', fmt='.', label = 'sci fiber')\n",
    "                \n",
    "                \n",
    "                ax1.set_title('x position of ThAr cal and sci points '+ str(cal[i].dropna()['xcentroid'][0]))\n",
    "                ax2.set_title('y position of ThAr cal and sci points ' + str(cal[i].dropna()['ycentroid'][0]))\n",
    "                ax3.set_title('wavelength of feature ' + str(cal[i].dropna()['wavelength'][0]))\n",
    "                \n",
    "                ax1.legend()\n",
    "                ax2.legend()\n",
    "                \n",
    "                ax1.tick_params(axis = 'x', rotation=70, labelsize=9)\n",
    "                ax2.tick_params(axis = 'x', rotation=70, labelsize=9)\n",
    "                ax3.tick_params(axis = 'x', rotation=70, labelsize=9)\n",
    "                plt.setp((ax1, ax2), ylabel='pixel value-median', xlabel='Date Obs')\n",
    "                plt.setp(ax3, ylabel = 'wavelength - median')\n",
    "                plt.show()\n",
    "                \n",
    "    def do_all(self, coords, vmax=0.3, vmin=-0.3, x_range = (0,1), cmin=0, cmax=0.5):\n",
    "        self.violin_plot(vmin=vmin, vmax=vmax)\n",
    "        self.centroid_rmse_plot(x_range=x_range)\n",
    "        self.centroid_std_map(cmin=cmin, cmax=cmax)\n",
    "        self.plot_feature_coords(coords)\n",
    "        self.features_std()\n",
    "        self.wavelength_std_plot()\n",
    "        self.rvprecsn()\n",
    "        self.cal_and_sci(coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9c076c",
   "metadata": {},
   "source": [
    "# Use case 1: Start from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3bcedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List DS9 x-y coordinates of well resolved features picked from various areas of the image\n",
    "ds9_xy = [[2014, 2175], [2104, 2173], [1686, 2119], [1378, 2273], [1130, 2363], [2207, 3256], [2133, 3169], [3040, 2607], [3185, 2613], [2008, 3088], [738, 3647], [940, 3435], [3164, 3529], [2210, 1974], [1827, 1919], [1860, 1855], [1659, 1681], [1936, 3539], [2248, 3915], [3086, 2119]]\n",
    "\n",
    "#more than 30 days is not recommended (to save your RAM)\n",
    "test = SQAT()\n",
    "\n",
    "test.download_data(start_date = '2022-09-01', end_date = '2022-09-02', site = 'lsc', doubles_path = '/home/pkottapalli/nres_tests/chile_sept_doubles/', lampflats_path = '/home/pkottapalli/nres_tests/lampflats/chile_sept_flats/')\n",
    "test.setup_pipeline(processed_path = '~/SQAT_toolkit', db_path = None) #Sets up db and BANZAI context, and loads in images\n",
    "test.run_pipeline() #runs feature identification, wavelength calibration, and wavelength qc\n",
    "\n",
    "test.get_data() #Open fits files and gets all the data we might need\n",
    "\n",
    "#new and old refer to the new feature tables and the old feature tables.\n",
    "test.get_sources_new(path = '~/nres_tests/new_features/lsc/nres01/202209*/processed') #Collates sources, result is a list of pandas tables, each table is a source in every image.\n",
    "test.get_some_features(ds9_xy) #finds source tables for requested coordinates\n",
    "test.do_all(ds9_xy) #Plots all available plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9868407",
   "metadata": {},
   "source": [
    "# Use case 2: To use your own files already downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqat = SQAT()\n",
    "# format of coords [[x1, y1], [x2,y2],...]\n",
    "#for single coordinate do [[x1, y1]]\n",
    "ds9_xy = [[2031, 2179],[2121, 2177], [1702, 2124], [1409, 2141], [1203, 2294], [2231, 3265], [2148, 3158], [3053, 2537], [3059, 2612], [2031, 3098], [1352, 3500], [963, 3448], [3191, 3535], [2226, 1977], [1841, 1923], [1861, 1859], [1673, 1684], [1960, 3550], [2276, 3929], [3100, 2121]]\n",
    "\n",
    "sqat.get_data(path='~/nres_tests/doubles/')\n",
    "sqat.get_sources_old()\n",
    "sqat2.get_some_features(ds9_xy)\n",
    "sqat.do_all(coords = ds9_xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2735fa2f",
   "metadata": {},
   "source": [
    "# Available convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0900893",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search for one source given ds9 coordinates\n",
    "sqat.get_some_features([[2031,2179]])\n",
    "\n",
    "#convert coordinates from ds9 to python\n",
    "new_coords = sqat.ds9_to_python([[2031,2179]])\n",
    "\n",
    "#run image segmentation on provided files\n",
    "sqat.stacked_files = '/path/to/files'\n",
    "sqat.extract() #returns list of source tables\n",
    "sqat.signaltonoise(image_data) #Calculates signal to noise for an image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c72c7",
   "metadata": {},
   "source": [
    "# Available convenience variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b21f2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After running setup_pipeline()\n",
    "test.context #Banzai context object\n",
    "test.frame_factory #NRES Frame Factory object\n",
    "\n",
    "#After running get_data()\n",
    "sqat.stacked_files #list of provided files\n",
    "sqat.header\n",
    "sqat.times #mmdd formatted times\n",
    "\n",
    "#After running get_sources\n",
    "sqat.features #list of feature tables from images\n",
    "sqat.sources #a list of tables where each table is one source over every image provided. missing values are NaNs\n",
    "\n",
    "#After running get_some_features()\n",
    "sqat.found #the sources tables found after searching for sources at specific DS9 coordinates\n",
    "\n",
    "#After running rvprecsn()\n",
    "sqat.rv #calculated rv precision for the set of images provided"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c22587",
   "metadata": {},
   "source": [
    "# Use case 3: Use BANZAI utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b697e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sigma clipped mean of an image\n",
    "from banzai.utils.stats import sigma_clipped_mean\n",
    "\n",
    "sm_mean = sigma_clipped_mean(fits.open(sqat2.stacked_files[0])['SPECTRUM'].data, sigma = 3)\n",
    "print(sm_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68dfa41",
   "metadata": {},
   "source": [
    "# Use case 4: Use data to explore on your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c480da0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot de-trended centroids and wavelengths\n",
    "for f in sqat.found:\n",
    "    x_diffs = []\n",
    "    y_diffs = []\n",
    "    wave_diffs = []\n",
    "    x = f['xcentroid']\n",
    "    y = f['ycentroid']\n",
    "    wave = f['wavelength']\n",
    "    for i in range(1, len(x)):\n",
    "        xval = x[i]-x[i-1]\n",
    "        yval = y[i]-y[i-1]\n",
    "        waveval = wave[i]-wave[i-1]\n",
    "        x_diffs.append(xval)\n",
    "        y_diffs.append(yval)\n",
    "        wave_diffs.append(waveval)\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,  3, figsize=(20,5), sharey=False)\n",
    "    t = f['date-obs']\n",
    "    ax1.errorbar(t[1:], x_diffs, yerr = None, ms = 6, fmt='.')\n",
    "    ax1.hlines(y = np.mean(x_diffs), xmin = t[0], xmax = t[1], color='r', linestyle='--')\n",
    "    ax2.errorbar(t[1:], y_diffs, yerr = None, ms = 6, fmt='.')\n",
    "    ax2.hlines(y = np.mean(y_diffs), xmin = t[0], xmax = t[1], color='r', linestyle='--')\n",
    "    ax3.errorbar(t[1:], wave_diffs, yerr = None, ms = 6, fmt='.')\n",
    "    ax3.hlines(y = np.mean(wave_diffs), xmin = t[0], xmax = t[1], color='r', linestyle='--')\n",
    "    \n",
    "    ax1.set_title('detrended x position of ThAr cal points '+ str(f.dropna()['xcentroid'][0])[:-5])\n",
    "    ax2.set_title('detrended y position of ThAr cal points ' + str(f.dropna()['ycentroid'][0])[:-5])\n",
    "    ax3.set_title('wavelength of feature ' + str(f.dropna()['wavelength'][0])[:-5])\n",
    "    ax1.tick_params(axis = 'x', rotation=70, labelsize=9)\n",
    "    ax2.tick_params(axis = 'x', rotation=70, labelsize=9)\n",
    "    ax3.tick_params(axis = 'x', rotation=70, labelsize=9)\n",
    "    plt.setp((ax1, ax2), ylabel='difference from previous point', )\n",
    "    plt.setp(ax3, ylabel = 'wavelength - median')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
